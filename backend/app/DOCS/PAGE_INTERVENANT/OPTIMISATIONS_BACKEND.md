# ğŸš€ Plan d'Optimisations Backend - Simulateur RH

## ğŸ“‹ Vue d'ensemble

Ce document prÃ©sente un plan d'optimisation complet pour amÃ©liorer les performances, la scalabilitÃ© et la maintenabilitÃ© du backend du Simulateur RH.

---

## ğŸ¯ Objectifs

1. **Performance** : RÃ©duire les temps de rÃ©ponse de 50-70%
2. **ScalabilitÃ©** : Supporter 10x plus d'utilisateurs simultanÃ©s
3. **MaintenabilitÃ©** : Code plus propre et testable
4. **CoÃ»ts** : RÃ©duire la charge serveur et base de donnÃ©es

---

## ğŸ“Š Analyse de l'existant

### Points critiques identifiÃ©s :
- âœ… RequÃªtes SQL multiples pour charger les rÃ©fÃ©rentiels
- âœ… Calculs de simulation lourds effectuÃ©s de maniÃ¨re synchrone
- âœ… Pas de cache pour les donnÃ©es rÃ©fÃ©rentielles
- âœ… Jointures SQL non optimisÃ©es dans plusieurs endpoints

---

## ğŸ”§ Optimisations ProposÃ©es

### 1ï¸âƒ£ **Cache Redis pour les RÃ©fÃ©rentiels** (PrioritÃ©: HAUTE)

#### BÃ©nÃ©fices attendus :
- âš¡ RÃ©duction de 80% du temps de chargement des rÃ©fÃ©rentiels
- ğŸ“‰ Diminution de la charge DB de 60%
- ğŸš€ AmÃ©lioration de l'expÃ©rience utilisateur

#### ImplÃ©mentation :

```python
# backend/app/core/cache.py
import redis
from functools import wraps
import json
import hashlib
from typing import Optional, Callable, Any
from app.core.config import settings

# Configuration Redis
redis_client = redis.Redis(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=settings.REDIS_DB,
    decode_responses=True
)

def cache_key(*args, **kwargs) -> str:
    """GÃ©nÃ¨re une clÃ© de cache unique basÃ©e sur les arguments"""
    key_data = f"{args}_{kwargs}"
    return hashlib.md5(key_data.encode()).hexdigest()

def redis_cache(ttl: int = 3600, prefix: str = ""):
    """
    DÃ©corateur pour mettre en cache les rÃ©sultats de fonction dans Redis
    
    Args:
        ttl: DurÃ©e de vie du cache en secondes (dÃ©faut: 1h)
        prefix: PrÃ©fixe pour la clÃ© de cache
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            # GÃ©nÃ©rer la clÃ© de cache
            key = f"{prefix}:{func.__name__}:{cache_key(*args, **kwargs)}"
            
            # Tenter de rÃ©cupÃ©rer depuis le cache
            try:
                cached = redis_client.get(key)
                if cached:
                    return json.loads(cached)
            except Exception as e:
                print(f"Cache read error: {e}")
            
            # ExÃ©cuter la fonction si pas en cache
            result = func(*args, **kwargs)
            
            # Stocker dans le cache
            try:
                redis_client.setex(
                    key,
                    ttl,
                    json.dumps(result, default=str)
                )
            except Exception as e:
                print(f"Cache write error: {e}")
            
            return result
        return wrapper
    return decorator

def invalidate_cache(pattern: str):
    """Invalide tous les caches correspondant au pattern"""
    try:
        for key in redis_client.scan_iter(match=pattern):
            redis_client.delete(key)
    except Exception as e:
        print(f"Cache invalidation error: {e}")
```

#### Utilisation dans les services :

```python
# backend/app/services/referentiel_service.py
from app.core.cache import redis_cache, invalidate_cache
from sqlalchemy.orm import Session
from sqlalchemy import text

@redis_cache(ttl=7200, prefix="ref")  # Cache 2h
def get_referentiel_taches(db: Session, centre_id: int, poste_id: int):
    """
    RÃ©cupÃ¨re le rÃ©fÃ©rentiel des tÃ¢ches avec cache Redis
    """
    sql = text("""
        SELECT 
            t.id, t.nom_tache, t.phase, t.unite_mesure, 
            t.moyenne_min, t.centre_poste_id,
            p.code as poste_code, p.nom_poste, p.type_poste
        FROM dbo.taches t
        JOIN dbo.centre_poste cp ON t.centre_poste_id = cp.id
        JOIN dbo.postes p ON cp.poste_id = p.id
        WHERE cp.centre_id = :centre_id 
        AND cp.poste_id = :poste_id
    """)
    
    result = db.execute(sql, {"centre_id": centre_id, "poste_id": poste_id})
    return [dict(row._mapping) for row in result]

@redis_cache(ttl=3600, prefix="ref")  # Cache 1h
def get_centres_by_direction(db: Session, direction_id: int):
    """RÃ©cupÃ¨re les centres d'une direction avec cache"""
    sql = text("""
        SELECT c.id, c.label, c.region_id
        FROM dbo.centres c
        WHERE c.direction_id = :direction_id
        ORDER BY c.label
    """)
    
    result = db.execute(sql, {"direction_id": direction_id})
    return [dict(row._mapping) for row in result]

def invalidate_referentiel_cache(centre_id: Optional[int] = None):
    """Invalide le cache des rÃ©fÃ©rentiels"""
    if centre_id:
        invalidate_cache(f"ref:*centre_id*{centre_id}*")
    else:
        invalidate_cache("ref:*")
```

---

### 2ï¸âƒ£ **Optimisation des RequÃªtes SQL** (PrioritÃ©: HAUTE)

#### ProblÃ¨me actuel :
RequÃªtes multiples en cascade (N+1 queries)

#### Solution : Jointures optimisÃ©es

```python
# backend/app/services/optimized_queries.py
from sqlalchemy import text
from sqlalchemy.orm import Session
from typing import List, Dict

def get_direction_complete_data(db: Session, direction_id: int) -> Dict:
    """
    RÃ©cupÃ¨re toutes les donnÃ©es d'une direction en UNE SEULE requÃªte
    au lieu de multiples requÃªtes en cascade
    """
    sql = text("""
        WITH DirectionInfo AS (
            SELECT id, label, region_id
            FROM dbo.directions
            WHERE id = :direction_id
        ),
        CentresData AS (
            SELECT 
                c.id as centre_id,
                c.label as centre_label,
                c.region_id,
                cp.id as centre_poste_id,
                cp.poste_id,
                cp.effectif_actuel,
                p.code as poste_code,
                p.nom_poste,
                p.type_poste
            FROM dbo.centres c
            JOIN dbo.centre_poste cp ON c.id = cp.centre_id
            JOIN dbo.postes p ON cp.poste_id = p.id
            WHERE c.direction_id = :direction_id
        ),
        TachesData AS (
            SELECT 
                t.*,
                cd.centre_id,
                cd.poste_code,
                cd.nom_poste,
                cd.type_poste,
                cd.effectif_actuel
            FROM dbo.taches t
            JOIN CentresData cd ON t.centre_poste_id = cd.centre_poste_id
        )
        SELECT 
            di.id as direction_id,
            di.label as direction_label,
            di.region_id,
            td.*
        FROM DirectionInfo di
        CROSS JOIN TachesData td
    """)
    
    result = db.execute(sql, {"direction_id": direction_id})
    rows = [dict(row._mapping) for row in result]
    
    # Restructurer les donnÃ©es
    if not rows:
        return None
    
    direction_data = {
        "id": rows[0]["direction_id"],
        "label": rows[0]["direction_label"],
        "region_id": rows[0]["region_id"],
        "centres": {}
    }
    
    for row in rows:
        centre_id = row["centre_id"]
        if centre_id not in direction_data["centres"]:
            direction_data["centres"][centre_id] = {
                "id": centre_id,
                "label": row.get("centre_label"),
                "postes": {},
                "taches": []
            }
        
        direction_data["centres"][centre_id]["taches"].append({
            "id": row["id"],
            "nom_tache": row["nom_tache"],
            "phase": row["phase"],
            "moyenne_min": row["moyenne_min"],
            "unite_mesure": row["unite_mesure"],
            "poste_code": row["poste_code"],
            "type_poste": row["type_poste"]
        })
    
    return direction_data
```

---

### 3ï¸âƒ£ **Calculs Asynchrones avec Celery** (PrioritÃ©: MOYENNE)

#### Pour les simulations lourdes (Direction, Nationale)

```python
# backend/app/core/celery_app.py
from celery import Celery
from app.core.config import settings

celery_app = Celery(
    "simulateur_rh",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Europe/Paris',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5 minutes max
    task_soft_time_limit=240,  # Warning Ã  4 minutes
)
```

```python
# backend/app/tasks/simulation_tasks.py
from app.core.celery_app import celery_app
from app.services.direction_service import process_direction_simulation
from app.core.database import SessionLocal
from typing import Dict

@celery_app.task(bind=True, name="simulation.direction")
def async_direction_simulation(self, direction_id: int, request_data: Dict):
    """
    TÃ¢che asynchrone pour calculer une simulation de direction
    """
    db = SessionLocal()
    try:
        # Mettre Ã  jour le statut
        self.update_state(state='PROGRESS', meta={'progress': 0})
        
        # ExÃ©cuter la simulation
        result = process_direction_simulation(db, request_data)
        
        # Mettre Ã  jour le statut
        self.update_state(state='PROGRESS', meta={'progress': 100})
        
        return result
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
    finally:
        db.close()

@celery_app.task(bind=True, name="simulation.nationale")
def async_nationale_simulation(self, request_data: Dict):
    """
    TÃ¢che asynchrone pour calculer une simulation nationale
    """
    db = SessionLocal()
    try:
        # Simulation nationale (trÃ¨s lourde)
        self.update_state(state='PROGRESS', meta={'progress': 0})
        
        # TODO: ImplÃ©menter la logique nationale
        result = {"status": "completed"}
        
        self.update_state(state='PROGRESS', meta={'progress': 100})
        return result
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
    finally:
        db.close()
```

```python
# backend/app/api/simulation_async.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.tasks.simulation_tasks import async_direction_simulation
from celery.result import AsyncResult

router = APIRouter()

@router.post("/simulation/direction/{direction_id}/async")
def start_direction_simulation_async(
    direction_id: int,
    request_data: dict,
    db: Session = Depends(get_db)
):
    """
    Lance une simulation de direction en mode asynchrone
    Retourne un task_id pour suivre la progression
    """
    task = async_direction_simulation.delay(direction_id, request_data)
    
    return {
        "task_id": task.id,
        "status": "PENDING",
        "message": "Simulation lancÃ©e en arriÃ¨re-plan"
    }

@router.get("/simulation/task/{task_id}")
def get_simulation_status(task_id: str):
    """
    RÃ©cupÃ¨re le statut d'une simulation asynchrone
    """
    task = AsyncResult(task_id)
    
    if task.state == 'PENDING':
        response = {
            "state": task.state,
            "status": "En attente..."
        }
    elif task.state == 'PROGRESS':
        response = {
            "state": task.state,
            "progress": task.info.get('progress', 0),
            "status": "Calcul en cours..."
        }
    elif task.state == 'SUCCESS':
        response = {
            "state": task.state,
            "result": task.result,
            "status": "TerminÃ©"
        }
    elif task.state == 'FAILURE':
        response = {
            "state": task.state,
            "error": str(task.info),
            "status": "Erreur"
        }
    else:
        response = {
            "state": task.state,
            "status": "Ã‰tat inconnu"
        }
    
    return response
```

---

### 4ï¸âƒ£ **Pool de Connexions DB OptimisÃ©** (PrioritÃ©: MOYENNE)

```python
# backend/app/core/database.py
from sqlalchemy import create_engine, event
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

# Configuration optimisÃ©e du pool de connexions
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # Nombre de connexions permanentes
    max_overflow=10,       # Connexions supplÃ©mentaires en pic
    pool_timeout=30,       # Timeout pour obtenir une connexion
    pool_recycle=3600,     # Recycler les connexions aprÃ¨s 1h
    pool_pre_ping=True,    # VÃ©rifier la connexion avant utilisation
    echo=False,
    connect_args={
        "timeout": 30,
        "check_same_thread": False
    }
)

# Ã‰vÃ©nement pour optimiser les requÃªtes
@event.listens_for(engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):
    """Log des requÃªtes lentes"""
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, params, context, executemany):
    """DÃ©tection des requÃªtes lentes"""
    total = time.time() - conn.info['query_start_time'].pop()
    if total > 1.0:  # RequÃªtes > 1s
        logger.warning(f"Slow query ({total:.2f}s): {statement[:100]}")
```

---

### 5ï¸âƒ£ **Compression des RÃ©ponses** (PrioritÃ©: BASSE)

```python
# backend/app/main.py
from fastapi import FastAPI
from fastapi.middleware.gzip import GZipMiddleware

app = FastAPI()

# Compression GZIP pour rÃ©duire la taille des rÃ©ponses
app.add_middleware(
    GZipMiddleware,
    minimum_size=1000,  # Compresser si > 1KB
    compresslevel=6     # Niveau de compression (1-9)
)
```

---

### 6ï¸âƒ£ **Pagination et Limitation** (PrioritÃ©: MOYENNE)

```python
# backend/app/api/pagination.py
from fastapi import Query
from typing import Optional, List, TypeVar, Generic
from pydantic import BaseModel

T = TypeVar('T')

class PaginatedResponse(BaseModel, Generic[T]):
    items: List[T]
    total: int
    page: int
    page_size: int
    total_pages: int

def paginate(
    query,
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100)
):
    """
    Applique la pagination Ã  une requÃªte SQLAlchemy
    """
    total = query.count()
    items = query.offset((page - 1) * page_size).limit(page_size).all()
    
    return PaginatedResponse(
        items=items,
        total=total,
        page=page,
        page_size=page_size,
        total_pages=(total + page_size - 1) // page_size
    )
```

---

## ğŸ“¦ DÃ©pendances Ã  Ajouter

```txt
# Cache
redis==5.0.1
hiredis==2.3.2

# TÃ¢ches asynchrones
celery==5.3.4
celery[redis]==5.3.4

# Monitoring
prometheus-client==0.19.0
```

---

## ğŸ”„ Plan de Migration

### Phase 1 : Cache Redis (Semaine 1)
1. âœ… Installer Redis
2. âœ… CrÃ©er le module de cache
3. âœ… Appliquer le cache aux rÃ©fÃ©rentiels
4. âœ… Tests et validation

### Phase 2 : Optimisation SQL (Semaine 2)
1. âœ… Identifier les requÃªtes N+1
2. âœ… RÃ©Ã©crire avec jointures
3. âœ… Ajouter des index si nÃ©cessaire
4. âœ… Tests de performance

### Phase 3 : Celery (Semaine 3)
1. âœ… Setup Celery + Redis
2. âœ… Migrer simulations lourdes
3. âœ… Interface de suivi des tÃ¢ches
4. âœ… Tests de charge

### Phase 4 : Optimisations diverses (Semaine 4)
1. âœ… Pool de connexions
2. âœ… Compression GZIP
3. âœ… Pagination
4. âœ… Monitoring

---

## ğŸ“ˆ MÃ©triques de SuccÃ¨s

| MÃ©trique | Avant | Objectif | MÃ©thode de mesure |
|----------|-------|----------|-------------------|
| Temps rÃ©ponse rÃ©fÃ©rentiels | ~500ms | <100ms | Logs + APM |
| Temps simulation Direction | ~3s | <1s | Logs + APM |
| RequÃªtes DB/requÃªte | ~15 | <5 | Query counter |
| Charge CPU | 60% | <30% | Monitoring serveur |
| Utilisateurs simultanÃ©s | 10 | 100+ | Tests de charge |

---

## ğŸ§ª Tests de Performance

```python
# backend/tests/performance/test_cache.py
import pytest
import time
from app.services.referentiel_service import get_referentiel_taches

def test_cache_performance(db_session):
    """VÃ©rifie que le cache amÃ©liore les performances"""
    
    # Premier appel (sans cache)
    start = time.time()
    result1 = get_referentiel_taches(db_session, centre_id=1, poste_id=1)
    time_without_cache = time.time() - start
    
    # DeuxiÃ¨me appel (avec cache)
    start = time.time()
    result2 = get_referentiel_taches(db_session, centre_id=1, poste_id=1)
    time_with_cache = time.time() - start
    
    # Le cache doit Ãªtre au moins 5x plus rapide
    assert time_with_cache < time_without_cache / 5
    assert result1 == result2
```

---

## ğŸš¨ Points d'Attention

1. **Redis** : PrÃ©voir un plan de backup et de haute disponibilitÃ©
2. **Celery** : Monitorer la file d'attente pour Ã©viter les engorgements
3. **Cache** : StratÃ©gie d'invalidation claire pour Ã©viter les donnÃ©es obsolÃ¨tes
4. **SQL** : Tester les performances avec des volumes de donnÃ©es rÃ©els

---

## ğŸ“š Ressources

- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [Celery Documentation](https://docs.celeryq.dev/)
- [SQLAlchemy Performance](https://docs.sqlalchemy.org/en/20/faq/performance.html)
- [FastAPI Performance](https://fastapi.tiangolo.com/advanced/performance/)

---

## âœ… Checklist de DÃ©ploiement

- [ ] Redis installÃ© et configurÃ©
- [ ] Celery worker dÃ©marrÃ©
- [ ] Variables d'environnement configurÃ©es
- [ ] Tests de performance validÃ©s
- [ ] Monitoring en place
- [ ] Documentation mise Ã  jour
- [ ] Formation Ã©quipe effectuÃ©e

---

**Date de crÃ©ation** : 26/12/2024  
**DerniÃ¨re mise Ã  jour** : 26/12/2024  
**Auteur** : Ã‰quipe Technique Simulateur RH
